import asyncio
import io
from datetime import datetime, date, timezone, timedelta
from typing import Any, Dict, List, Optional, Set
from io import StringIO

import pandas as pd
import requests
import yfinance as yf
from bs4 import BeautifulSoup
from requests_cache import CachedSession
from sqlalchemy import delete, select
from sqlalchemy.ext.asyncio import AsyncSession

from app import models
from app.config import settings
from app.services import news_service, stock_service
from app.services.ai_service import ai_client

# í™˜ìœ¨ ìºì‹œ (í†µí™” â†’ USD í™˜ì‚°ìœ¨)
EXCHANGE_RATE_CACHE: Dict[str, float] = {}

# êµ­ê°€ë³„ ì•¼í›„ íŒŒì´ë‚¸ìŠ¤ í‹°ì»¤ ì ‘ë¯¸ì‚¬ ë§µ
COUNTRY_SUFFIX_MAP = {
    "United Kingdom": ".L",
    "Japan": ".T",
    "China": ".SS",  # ê¸°ë³¸ê°’ì€ ìƒí•´, ìˆ«ì ê·œì¹™ì€ ë³„ë„ ì²˜ë¦¬
    "Germany": ".DE",
    "France": ".PA",
    "Hong Kong": ".HK",
    "Canada": ".TO",
    "Australia": ".AX",
    "Taiwan": ".TW",
    "South Korea": ".KS",
    "India": ".NS",
}


def _apply_country_suffix(ticker: str, country: Optional[str]) -> str:
    """
    êµ­ê°€ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í‹°ì»¤ì— ì ‘ë¯¸ì‚¬ë¥¼ ìë™ ë¶€ì°©í•©ë‹ˆë‹¤.
    - ì´ë¯¸ '.'ì„ í¬í•¨í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    - ì¤‘êµ­: 6ìë¦¬ ìˆ«ì í‹°ì»¤ì¼ ë•Œ 6/9 ì‹œì‘ -> .SS, 0/3 ì‹œì‘ -> .SZ
    - ê·¸ ì™¸ COUNTRY_SUFFIX_MAPì— ì¡´ì¬í•˜ë©´ í•´ë‹¹ ì ‘ë¯¸ì‚¬ ë¶€ì°©
    """
    if not ticker:
        return ticker
    ticker = ticker.upper()

    # ì´ë¯¸ ì ‘ë¯¸ì‚¬ê°€ ìˆëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
    if "." in ticker:
        return ticker

    if country == "China":
        if ticker.isdigit() and len(ticker) == 6:
            if ticker.startswith(("0", "3")):
                return f"{ticker}.SZ"
            if ticker.startswith(("6", "9")):
                return f"{ticker}.SS"
        # ê·œì¹™ì— ë§ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì ‘ë¯¸ì‚¬ ì ìš©
        suffix = COUNTRY_SUFFIX_MAP.get(country)
        return f"{ticker}{suffix}" if suffix else ticker

    suffix = COUNTRY_SUFFIX_MAP.get(country or "")
    return f"{ticker}{suffix}" if suffix else ticker


async def get_usd_exchange_rate(currency: Optional[str]) -> float:
    """
    ì£¼ì–´ì§„ í†µí™” 1ë‹¨ìœ„ê°€ USDë¡œ ì–¼ë§ˆì¸ì§€ í™˜ìœ¨ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    - USDëŠ” 1.0
    - ìºì‹œ ì‚¬ìš©
    - yfinance ì‹¬ë³¼:
        1) USD{currency}=X â†’ last_priceê°€ 1 USDë‹¹ í˜„ì§€í†µí™”ì´ë¯€ë¡œ ì—­ìˆ˜ ì‚¬ìš©
        2) {currency}USD=X â†’ last_price ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - ëª¨ë‘ ì‹¤íŒ¨ ì‹œ 1.0 ë°˜í™˜
    """
    if not currency:
        return 1.0

    currency = currency.upper()
    if currency == "USD":
        return 1.0

    if currency in EXCHANGE_RATE_CACHE:
        return EXCHANGE_RATE_CACHE[currency]

    def _fetch_rate_sync() -> float:
        # ì¼€ì´ìŠ¤ 1: USD{currency}=X (ì˜ˆ: USDJPY=X) â†’ 1 / last_price
        try:
            pair = f"USD{currency}=X"
            ticker = yf.Ticker(pair)
            last_price = None
            if ticker.fast_info and ticker.fast_info.last_price is not None:
                last_price = float(ticker.fast_info.last_price)
            elif ticker.info:
                last_price = ticker.info.get("regularMarketPrice") or ticker.info.get("lastPrice")
            if last_price:
                return 1.0 / float(last_price)
        except Exception:
            pass

        # ì¼€ì´ìŠ¤ 2: {currency}USD=X (ì˜ˆ: EURUSD=X) â†’ last_price
        try:
            pair = f"{currency}USD=X"
            ticker = yf.Ticker(pair)
            last_price = None
            if ticker.fast_info and ticker.fast_info.last_price is not None:
                last_price = float(ticker.fast_info.last_price)
            elif ticker.info:
                last_price = ticker.info.get("regularMarketPrice") or ticker.info.get("lastPrice")
            if last_price:
                return float(last_price)
        except Exception:
            pass

        # ì‹¤íŒ¨ ì‹œ fallback
        return 1.0

    rate = await asyncio.to_thread(_fetch_rate_sync)
    EXCHANGE_RATE_CACHE[currency] = rate
    return rate

# ìœ„í‚¤í”¼ë””ì•„ ìš”ì²­ìš© ìºì‹œ ì„¸ì…˜ (24ì‹œê°„ TTL - ìœ„í‚¤í”¼ë””ì•„ ë°ì´í„°ëŠ” ìì£¼ ë³€ê²½ë˜ì§€ ì•ŠìŒ)
_wiki_cache_session: Optional[CachedSession] = None


def _get_wiki_session() -> requests.Session:
    """
    ìœ„í‚¤í”¼ë””ì•„ ìš”ì²­ìš© ìºì‹œëœ ì„¸ì…˜ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì¤‘ë³µ í˜¸ì¶œì„ ë°©ì§€í•˜ì—¬ Rate Limitì„ í”¼í•©ë‹ˆë‹¤.
    """
    global _wiki_cache_session
    if _wiki_cache_session is None:
        _wiki_cache_session = CachedSession(
            cache_name='wikipedia_cache',
            backend='sqlite',
            expire_after=86400,  # 24ì‹œê°„
            cache_control=True,
        )
        _wiki_cache_session.headers.update({
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120.0.0.0 Safari/537.36"
            )
        })
    return _wiki_cache_session


_fmp_session: Optional[requests.Session] = None


def _get_fmp_session() -> requests.Session:
    """
    FMP ì´ë¯¸ì§€ ìš”ì²­ìš© ì„¸ì…˜ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    Keep-Aliveë¥¼ í™œìš©í•´ ì—°ê²° ì˜¤ë²„í—¤ë“œë¥¼ ì¤„ì…ë‹ˆë‹¤.
    """
    global _fmp_session
    if _fmp_session is None:
        session = requests.Session()
        session.headers.update({
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/122.0.0.0 Safari/537.36"
            ),
            "Accept": "image/avif,image/webp,image/apng,*/*;q=0.8",
            "Connection": "keep-alive",
        })
        _fmp_session = session
    return _fmp_session


def _search_japanese_ticker(company_name: str) -> Optional[str]:
    """
    íšŒì‚¬ ì´ë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ ì¼ë³¸ ì£¼ì‹ í‹°ì»¤(.Të¡œ ëë‚˜ëŠ”)ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    Yahoo Finance Search APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰í•˜ê³ , .Të¡œ ëë‚˜ëŠ” í‹°ì»¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        company_name: íšŒì‚¬ ì´ë¦„ (ì˜ˆ: "Toyota Motor Corporation")
        
    Returns:
        ì¼ë³¸ ì£¼ì‹ í‹°ì»¤ (ì˜ˆ: "7203.T") ë˜ëŠ” None
    """
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        # íšŒì‚¬ ì´ë¦„ ì •ì œ (ê´„í˜¸, íŠ¹ìˆ˜ ë¬¸ì ì œê±°)
        clean_name = company_name.split('(')[0].strip()
        if not clean_name:
            return None
        
        # Yahoo Finance Search API ì‚¬ìš©
        # yfinanceëŠ” ì§ì ‘ ê²€ìƒ‰ ê¸°ëŠ¥ì´ ì—†ìœ¼ë¯€ë¡œ, Yahoo Financeì˜ ê²€ìƒ‰ APIë¥¼ ì§ì ‘ í˜¸ì¶œ
        search_url = "https://query2.finance.yahoo.com/v1/finance/search"
        params = {"q": clean_name}
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        resp = requests.get(search_url, params=params, headers=headers, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        
        quotes = data.get("quotes", [])
        if not quotes:
            return None
        
        # .Të¡œ ëë‚˜ëŠ” í‹°ì»¤ ì°¾ê¸° (ì¼ë³¸ ì£¼ì‹)
        japanese_tickers = [
            quote.get("symbol") 
            for quote in quotes 
            if quote.get("symbol", "").endswith(".T")
        ]
        
        if not japanese_tickers:
            return None
        
        # ì—¬ëŸ¬ ê°œê°€ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ê²ƒ ì„ íƒ
        # (ì¼ë°˜ì ìœ¼ë¡œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²°ê³¼ê°€ ì²« ë²ˆì§¸ì— ì˜´)
        selected_ticker = japanese_tickers[0]
        
        # ADRì´ ì•„ë‹Œ ì¼ë°˜ ì£¼ì‹ì„ ì„ í˜¸ (ADRì€ ë³´í†µ ".TO" ë“± ë‹¤ë¥¸ suffixë¥¼ ê°€ì§)
        # .Të¡œ ëë‚˜ëŠ” ê²ƒì€ ì¼ë°˜ì ìœ¼ë¡œ ì¼ë³¸ ì£¼ì‹ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        return selected_ticker
        
    except Exception as e:
        logger.debug(f"í‹°ì»¤ ê²€ìƒ‰ ì‹¤íŒ¨ ({company_name}): {e}")
        return None


def _verify_ticker_with_yfinance(ticker: str) -> bool:
    """
    yfinanceë¥¼ ì‚¬ìš©í•˜ì—¬ í‹°ì»¤ê°€ ìœ íš¨í•œì§€ í™•ì¸í•©ë‹ˆë‹¤.
    HTTP 404 ë“±ì˜ ì—ëŸ¬ ë°œìƒ ì‹œ Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        t = yf.Ticker(ticker)
        # fast_infoë¥¼ ë¨¼ì € í™•ì¸ (ì†ë„ ìµœì í™”)
        if t.fast_info and t.fast_info.last_price is not None:
            return True
        
        # fast_infoê°€ ì•ˆë  ê²½ìš° info í™•ì¸
        info = t.info
        if info and (info.get("marketCap") or info.get("currentPrice") or info.get("regularMarketPrice")):
            return True
            
        return False
    except Exception:
        return False


def _parse_nikkei_225(html_content: str, logger) -> tuple[List[str], List[str]]:
    """
    Nikkei 225 Wikipedia í˜ì´ì§€ì—ì„œ ì¢…ëª© í‹°ì»¤ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
    êµ¬ì¡°: <div class="mw-heading2"><h2>Components</h2></div> -> ... -> <h3>Sector</h3> -> <ul> -> <li>
    """
    import re
    from bs4 import BeautifulSoup, Tag
    
    resolved_tickers = []
    
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # 1. Components í—¤ë” ì°¾ê¸°
        header = soup.find(id="Components")
        if not header:
            # idë¡œ ëª» ì°¾ìœ¼ë©´ í…ìŠ¤íŠ¸ë¡œ ì‹œë„
            for h2 in soup.find_all('h2'):
                if "components" in h2.get_text().lower():
                    header = h2
                    break
        
        if not header:
            logger.warning("   âš ï¸ Nikkei 'Components' í—¤ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return [], []

        # ğŸš¨ [í•µì‹¬ ìˆ˜ì •] ìœ„í‚¤í”¼ë””ì•„ì˜ ìµœì‹  êµ¬ì¡° ëŒ€ì‘ (Vector 2022 skin)
        # <h2>ê°€ <div class="mw-heading ..."> ì•ˆì— ê°‡í˜€ ìˆëŠ” ê²½ìš°, ë¶€ëª¨ divë¡œ ì˜¬ë¼ê°
        start_node = header
        if header.parent and header.parent.name == 'div' and 'mw-heading' in header.parent.get('class', []):
            start_node = header.parent
            
        logger.info(f"   â„¹ï¸ Nikkei íŒŒì‹± ì‹œì‘ ë…¸ë“œ: {start_node.name} (class: {start_node.get('class')})")

        # 2. ë‹¤ìŒ ë©”ì¸ ì„¹ì…˜(H2)ì´ ë‚˜ì˜¬ ë•Œê¹Œì§€ í˜•ì œ ë…¸ë“œ ìˆœíšŒ
        # Components ì„¹ì…˜ ì•„ë˜ì—ëŠ” ì—¬ëŸ¬ ê°œì˜ <h3>(ì„¹í„°)ì™€ <ul>(ì¢…ëª© ë¦¬ìŠ¤íŠ¸)ì´ ì„ì—¬ ìˆìŒ
        for sibling in start_node.next_siblings:
            # Tag ê°ì²´ê°€ ì•„ë‹Œ ê²½ìš°(ê³µë°± ë¬¸ìì—´ ë“±) ê±´ë„ˆëœ€
            if not isinstance(sibling, Tag):
                continue

            # ë‹¤ìŒ ë©”ì¸ ì„¹ì…˜ í—¤ë”(H2)ë¥¼ ë§Œë‚˜ë©´ ì¢…ë£Œ
            # 1) <h2> íƒœê·¸ì¸ ê²½ìš°
            if sibling.name == 'h2':
                break
            # 2) <div class="mw-heading mw-heading2"> ì•ˆì— <h2>ê°€ ìˆëŠ” ê²½ìš°
            if sibling.name == 'div' and 'mw-heading2' in sibling.get('class', []):
                break
                
            # ë¦¬ìŠ¤íŠ¸(ul)ë¥¼ ë§Œë‚˜ë©´ íŒŒì‹±
            if sibling.name == 'ul':
                for li in sibling.find_all('li'):
                    text = li.get_text().strip() # ì˜ˆ: "ANA Holdings Inc. (TYO: 9202)"
                    
                    # ì •ê·œì‹: (TYO: 9202) íŒ¨í„´ ì°¾ê¸°
                    # ì´ë¯¸ì§€ì— ë”°ë¥´ë©´ ê´„í˜¸ ì•ˆì— TYO: ìˆ«ì íŒ¨í„´ì´ ìˆìŒ
                    match = re.search(r'TYO:\s*(\d{4})', text, re.IGNORECASE)
                    if match:
                        code = match.group(1)
                        ticker = f"{code}.T"
                        resolved_tickers.append(ticker)
                    else:
                        # ì˜ˆë¹„ íŒ¨í„´: ê´„í˜¸ ì•ˆì— 4ìë¦¬ ìˆ«ìë§Œ ìˆëŠ” ê²½ìš°
                        alt_match = re.search(r'\((\d{4})\)', text)
                        if alt_match:
                            code = alt_match.group(1)
                            ticker = f"{code}.T"
                            resolved_tickers.append(ticker)

    except Exception as e:
        logger.error(f"   âŒ Nikkei 225 íŒŒì‹± ì˜¤ë¥˜: {e}")
        import traceback
        logger.debug(traceback.format_exc())
        return [], []

    # ì¤‘ë³µ ì œê±°
    unique_tickers = sorted(list(set(resolved_tickers)))
    
    if unique_tickers:
        logger.info(f"   âœ… {len(unique_tickers)}ê°œ Nikkei í‹°ì»¤ ì¶”ì¶œ ì™„ë£Œ")
    else:
        logger.warning("   âš ï¸ Nikkei í‹°ì»¤ ì¶”ì¶œ ì‹¤íŒ¨ (0ê°œ)")

    return unique_tickers, []


def _parse_hang_seng_index(html_content: str, logger) -> tuple[List[str], List[str]]:
    """
    Hang Seng Index: ìˆ«ì í‹°ì»¤ë¥¼ 4ìë¦¬ë¡œ íŒ¨ë”©(zfill)í•˜ì—¬ .HK ë¶™ì„
    """
    import re
    try:
        html_io = StringIO(html_content)
        tables = pd.read_html(html_io)
        resolved_tickers = []
        unresolved_companies = []
        
        for table in tables:
            if isinstance(table.columns, pd.MultiIndex):
                table.columns = table.columns.get_level_values(0)
            
            ticker_column = next((col for col in table.columns if 'ticker' in str(col).lower() or 'symbol' in str(col).lower() or 'code' in str(col).lower()), None)
            
            if ticker_column is None:
                continue
            
            for _, row in table.iterrows():
                try:
                    ticker_raw = str(row[ticker_column]).strip()
                    if not ticker_raw or ticker_raw.lower() == 'nan':
                        continue
                    
                    # ìˆ«ìë§Œ ì¶”ì¶œ
                    digits = re.sub(r'\D', '', ticker_raw)
                    
                    if digits and len(digits) <= 5:
                        ticker = f"{digits.zfill(4)}.HK"
                        if _verify_ticker_with_yfinance(ticker):
                            resolved_tickers.append(ticker)
                        else:
                            unresolved_companies.append(ticker)
                    elif _verify_ticker_with_yfinance(ticker_raw):
                        resolved_tickers.append(ticker_raw)
                            
                except Exception:
                    continue
                    
        return list(set(resolved_tickers)), unresolved_companies
    except Exception as e:
        logger.error(f"   âŒ Hang Seng íŒŒì‹± ì˜¤ë¥˜: {e}")
        return [], []


def _search_hong_kong_ticker(company_name: str) -> Optional[str]:
    """
    íšŒì‚¬ ì´ë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ í™ì½© ì£¼ì‹ í‹°ì»¤(.HKë¡œ ëë‚˜ëŠ”)ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        company_name: íšŒì‚¬ ì´ë¦„
        
    Returns:
        í™ì½© ì£¼ì‹ í‹°ì»¤ (ì˜ˆ: "0700.HK") ë˜ëŠ” None
    """
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        clean_name = company_name.split('(')[0].strip()
        if not clean_name:
            return None
        
        # Yahoo Finance Search API ì‚¬ìš©
        search_url = "https://query2.finance.yahoo.com/v1/finance/search"
        params = {"q": clean_name}
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        resp = requests.get(search_url, params=params, headers=headers, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        
        quotes = data.get("quotes", [])
        if not quotes:
            return None
        
        # .HKë¡œ ëë‚˜ëŠ” í‹°ì»¤ ì°¾ê¸°
        hk_tickers = [
            quote.get("symbol") 
            for quote in quotes 
            if quote.get("symbol", "").endswith(".HK")
        ]
        
        if not hk_tickers:
            return None
        
        return hk_tickers[0]
        
    except Exception as e:
        logger.debug(f"í™ì½© í‹°ì»¤ ê²€ìƒ‰ ì‹¤íŒ¨ ({company_name}): {e}")
        return None


def _parse_csi_300_index(html_content: str, logger) -> tuple[List[str], List[str]]:
    """
    CSI 300: "SSE: 600519" ê°™ì€ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œí•˜ì—¬ .SS/.SZ ë§¤í•‘
    """
    import re
    try:
        html_io = StringIO(html_content)
        tables = pd.read_html(html_io)
        resolved_tickers = []
        unresolved_companies = []
        
        for table in tables:
            if isinstance(table.columns, pd.MultiIndex):
                table.columns = table.columns.get_level_values(0)
            
            ticker_column = next((col for col in table.columns if 'ticker' in str(col).lower() or 'symbol' in str(col).lower() or 'code' in str(col).lower()), None)
            
            if ticker_column is None:
                continue
            
            for _, row in table.iterrows():
                try:
                    raw_val = str(row[ticker_column])
                    match = re.search(r'(\d{6})', raw_val)
                    if not match:
                        continue
                    
                    ticker_digits = match.group(1)
                    
                    # ìƒí•´(SS) vs ì‹¬ì²œ(SZ) êµ¬ë¶„
                    if ticker_digits.startswith('6') or ticker_digits.startswith('9'):
                        suffix = '.SS'
                    else:
                        suffix = '.SZ'
                    
                    ticker = f"{ticker_digits}{suffix}"
                    
                    if _verify_ticker_with_yfinance(ticker):
                        resolved_tickers.append(ticker)
                    else:
                        # ì‹¤íŒ¨ ì‹œ ë°˜ëŒ€ ê±°ë˜ì†Œ ì‹œë„
                        alt_suffix = '.SZ' if suffix == '.SS' else '.SS'
                        alt_ticker = f"{ticker_digits}{alt_suffix}"
                        if _verify_ticker_with_yfinance(alt_ticker):
                            resolved_tickers.append(alt_ticker)
                                
                except Exception:
                    continue

        logger.info(f"   âœ… {len(resolved_tickers)}ê°œ CSI 300 í‹°ì»¤ ì¶”ì¶œ ì™„ë£Œ")
        return list(set(resolved_tickers)), []
    except Exception as e:
        logger.error(f"   âŒ CSI 300 íŒŒì‹± ì˜¤ë¥˜: {e}")
        return [], []


def _search_china_ticker(company_name: str) -> Optional[str]:
    """
    íšŒì‚¬ ì´ë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ ì¤‘êµ­ ì£¼ì‹ í‹°ì»¤(.SS ë˜ëŠ” .SZë¡œ ëë‚˜ëŠ”)ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        company_name: íšŒì‚¬ ì´ë¦„
        
    Returns:
        ì¤‘êµ­ ì£¼ì‹ í‹°ì»¤ (ì˜ˆ: "600519.SS" ë˜ëŠ” "000001.SZ") ë˜ëŠ” None
    """
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        clean_name = company_name.split('(')[0].strip()
        if not clean_name:
            return None
        
        # Yahoo Finance Search API ì‚¬ìš©
        search_url = "https://query2.finance.yahoo.com/v1/finance/search"
        params = {"q": clean_name}
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        resp = requests.get(search_url, params=params, headers=headers, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        
        quotes = data.get("quotes", [])
        if not quotes:
            return None
        
        # .SS ë˜ëŠ” .SZë¡œ ëë‚˜ëŠ” í‹°ì»¤ ì°¾ê¸°
        china_tickers = [
            quote.get("symbol") 
            for quote in quotes 
            if quote.get("symbol", "").endswith((".SS", ".SZ"))
        ]
        
        if not china_tickers:
            return None
        
        return china_tickers[0]
        
    except Exception as e:
        logger.debug(f"ì¤‘êµ­ í‹°ì»¤ ê²€ìƒ‰ ì‹¤íŒ¨ ({company_name}): {e}")
        return None


def _parse_dax(html_content: str, logger) -> tuple[List[str], List[str]]:
    """
    DAX Wikipedia í˜ì´ì§€ì—ì„œ .DEë¡œ ëë‚˜ëŠ” í‹°ì»¤ë¥¼ ì°¾ì•„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    í—¤ë” ì´ë¦„ ëŒ€ì‹  ë°ì´í„° íŒ¨í„´(Content-based)ì„ ì‚¬ìš©í•˜ì—¬ í…Œì´ë¸”ì„ ì°¾ìŠµë‹ˆë‹¤.
    """
    try:
        html_io = StringIO(html_content)
        # í—¤ë”ë¥¼ íŠ¹ì •í•˜ì§€ ì•Šê³  ë°ì´í„°ë¥¼ ëª¨ë‘ ì½ìŒ
        tables = pd.read_html(html_io, header=None)
        
        resolved_tickers = []
        unresolved_companies = []  # DAXëŠ” ë³´í†µ í‹°ì»¤ê°€ ëª…í™•í•˜ë¯€ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©
        
        found_table = False
        
        for table in tables:
            # í…Œì´ë¸”ì˜ ê° ì»¬ëŸ¼ì„ ìˆœíšŒí•˜ë©° .DE íŒ¨í„´ ê²€ì‚¬
            for col_idx in table.columns:
                # ìƒìœ„ 10ê°œ í–‰ë§Œ ìƒ˜í”Œë§í•˜ì—¬ íŒ¨í„´ í™•ì¸
                sample_values = table[col_idx].astype(str).head(10).tolist()
                
                # ".DE"ë¡œ ëë‚˜ëŠ” ë°ì´í„°ê°€ ì ˆë°˜ ì´ìƒì´ë©´ í‹°ì»¤ ì»¬ëŸ¼ìœ¼ë¡œ ê°„ì£¼
                de_matches = [v for v in sample_values if v.strip().endswith('.DE')]
                
                if len(de_matches) >= 3:  # ìµœì†Œ 3ê°œ ì´ìƒ ë§¤ì¹­ë˜ë©´ í™•ì‹ 
                    found_table = True
                    # í•´ë‹¹ ì»¬ëŸ¼ ì „ì²´ ë°ì´í„° ì¶”ì¶œ
                    tickers = table[col_idx].astype(str).tolist()
                    for ticker in tickers:
                        clean_ticker = ticker.strip()
                        if clean_ticker.endswith('.DE'):
                            resolved_tickers.append(clean_ticker)
                    break  # ì»¬ëŸ¼ ì°¾ìŒ -> í…Œì´ë¸” ë£¨í”„ ì¢…ë£Œ
            
            if found_table:
                break  # í…Œì´ë¸” ì°¾ìŒ -> ì „ì²´ ë£¨í”„ ì¢…ë£Œ
                
        if resolved_tickers:
            logger.info(f"   âœ… {len(resolved_tickers)}ê°œ DAX í‹°ì»¤ ì¶”ì¶œ ì™„ë£Œ (íŒ¨í„´ ë§¤ì¹­)")
            return list(set(resolved_tickers)), []
        else:
            logger.warning("   âš ï¸  DAX í…Œì´ë¸”ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (.DE íŒ¨í„´ ì—†ìŒ)")
            return [], []
            
    except Exception as e:
        logger.error(f"   âŒ DAX íŒŒì‹± ì˜¤ë¥˜: {e}")
        return [], []


# ì£¼ìš” ì§€ìˆ˜ ìœ„í‚¤í”¼ë””ì•„ í˜ì´ì§€ URL ëª©ë¡
WIKI_INDEX_SOURCES = [
    {
        "url": "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies",
        "suffix": "",
        "country": "United States",
    },
    # ìˆ˜ì§‘ëŸ‰ ê°ì†Œ ìš”ì²­ì— ë”°ë¼ NASDAQ-100 ì„ì‹œ ë¹„í™œì„±í™”
    # {
    #     "url": "https://en.wikipedia.org/wiki/NASDAQ-100",
    #     "suffix": "",
    #     "country": "United States",
    # },
    {
        "url": "https://en.wikipedia.org/wiki/CAC_40",
        "suffix": "",
        "country": "France",
    },
    {
        "url": "https://en.wikipedia.org/wiki/FTSE_100_Index",
        "suffix": "",
        "country": "United Kingdom",
    },
    {
        "url": "https://en.wikipedia.org/wiki/DAX",
        "suffix": "",
        "special_handling": "dax",  # DAXëŠ” íŠ¹ë³„ ì²˜ë¦¬ í•„ìš”
        "country": "Germany",
    },
    # {
    #     "url": "https://en.wikipedia.org/wiki/Nikkei_225",
    #     "suffix": "",
    #     "special_handling": "nikkei_225",  # Nikkei 225ëŠ” íŠ¹ë³„ ì²˜ë¦¬ í•„ìš”
    #     "country": "Japan",
    # },
    {
        "url": "https://en.wikipedia.org/wiki/Hang_Seng_Index",
        "suffix": "",
        "special_handling": "hang_seng",  # Hang Seng IndexëŠ” íŠ¹ë³„ ì²˜ë¦¬ í•„ìš”
        "country": "Hong Kong",
    },
    # {
    #     "url": "https://en.wikipedia.org/wiki/CSI_300_Index",
    #     "suffix": "",
    #     "special_handling": "csi_300",  # CSI 300 IndexëŠ” íŠ¹ë³„ ì²˜ë¦¬ í•„ìš”
    #     "country": "China",
    # },
]

# íŠ¹ë³„ ì²˜ë¦¬ ëŒ€ìƒ URL ìƒìˆ˜
NIKKEI_225_URL = "https://en.wikipedia.org/wiki/Nikkei_225"
HANG_SENG_URL = "https://en.wikipedia.org/wiki/Hang_Seng_Index"
CSI_300_URL = "https://en.wikipedia.org/wiki/CSI_300_Index"


async def fetch_index_tickers() -> Dict[str, str]:
    """
    ì£¼ìš” ì§€ìˆ˜(ì˜ˆ: S&P 500, NASDAQ-100 ë“±)ì˜ ìœ„í‚¤í”¼ë””ì•„ í˜ì´ì§€ì—ì„œ êµ¬ì„± ì¢…ëª© í‹°ì»¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    ìœ„í‚¤í”¼ë””ì•„ í…Œì´ë¸”ì„ íŒŒì‹±í•˜ì—¬ í‹°ì»¤ ì‹¬ë³¼ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Returns:
        {ticker: country} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
    """
    import logging
    logger = logging.getLogger(__name__)
    logger.info("ğŸ“‹ [Step 1] í‹°ì»¤ ìˆ˜ì§‘ ì‹œì‘: ìœ„í‚¤í”¼ë””ì•„ì—ì„œ ì£¼ìš” ì§€ìˆ˜ êµ¬ì„± ì¢…ëª© ìˆ˜ì§‘ ì¤‘...")
    logger.info(f"   ëŒ€ìƒ ì§€ìˆ˜: {len(WIKI_INDEX_SOURCES)}ê°œ")
    for idx, item in enumerate(WIKI_INDEX_SOURCES, 1):
        logger.info(f"   {idx}. {item['url']}")
    
    def _sync_job() -> tuple[Dict[str, str], dict]:
        import logging
        sync_logger = logging.getLogger(__name__)
        
        # ìºì‹œëœ ì„¸ì…˜ ì‚¬ìš© (ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€)
        session = _get_wiki_session()
        
        # 2) ìœ„í‚¤í”¼ë””ì•„ HTMLì—ì„œ í…Œì´ë¸” íŒŒì‹± (pandas.read_html ì‚¬ìš©)
        all_tickers_map: Dict[str, str] = {}
        success_count = 0
        fail_count = 0
        error_details = []
        
        for item in WIKI_INDEX_SOURCES:
            url = item["url"]
            suffix = item.get("suffix", "")
            special_handling = item.get("special_handling", "")
            country = item.get("country", "Unknown")
            is_nikkei_225 = special_handling == "nikkei_225" or url == NIKKEI_225_URL
            is_hang_seng = special_handling == "hang_seng" or url == HANG_SENG_URL
            is_csi_300 = special_handling == "csi_300" or url == CSI_300_URL
            is_dax = special_handling == "dax" or url == "https://en.wikipedia.org/wiki/DAX"
            
            try:
                # ìœ„í‚¤í”¼ë””ì•„ëŠ” User-Agentê°€ ì—†ìœ¼ë©´ 403 Forbiddenì„ ë°˜í™˜í•©ë‹ˆë‹¤
                # requestsë¡œ HTMLì„ ê°€ì ¸ì˜¨ í›„ pandas.read_htmlì— ì „ë‹¬
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                }
                
                # requestsë¡œ HTML ê°€ì ¸ì˜¤ê¸°
                resp = session.get(url, headers=headers, timeout=30)
                resp.raise_for_status()
                html_content = resp.text
                
                # Nikkei 225 íŠ¹ë³„ ì²˜ë¦¬
                if is_nikkei_225:
                    sync_logger.info(f"ğŸ‡¯ğŸ‡µ [Nikkei 225] íŠ¹ë³„ ì²˜ë¦¬ ì‹œì‘: Components ì„¹ì…˜ì—ì„œ ì¢…ëª© íŒŒì‹± ì¤‘...")
                    
                    try:
                        resolved_tickers, unresolved_companies = _parse_nikkei_225(html_content, sync_logger)
                        
                        # í•´ê²°ëœ í‹°ì»¤ ì¶”ê°€
                        for ticker in resolved_tickers:
                            all_tickers_map[ticker] = country
                        
                        tickers_from_this_source = len(resolved_tickers)
                        
                        if tickers_from_this_source > 0:
                            success_count += 1
                            error_details.append(
                                f"âœ… {url}: {tickers_from_this_source}ê°œ í‹°ì»¤ í•´ê²°"
                                + (f", {len(unresolved_companies)}ê°œ ë¯¸í•´ê²°" if unresolved_companies else "")
                            )
                        else:
                            fail_count += 1
                            error_details.append(
                                f"âš ï¸  {url}: í‹°ì»¤ í•´ê²° ì‹¤íŒ¨"
                                + (f" ({len(unresolved_companies)}ê°œ ì¢…ëª©)" if unresolved_companies else "")
                            )
                    except Exception as e:
                        fail_count += 1
                        import traceback
                        error_msg = f"âŒ {url}: Nikkei 225 íŒŒì‹± ì˜¤ë¥˜ - {type(e).__name__}: {str(e)[:200]}"
                        error_details.append(error_msg)
                        sync_logger.error(f"   {error_msg}")
                    
                    continue  # Nikkei 225 ì²˜ë¦¬ ì™„ë£Œ, ë‹¤ìŒ URLë¡œ
                
                # Hang Seng Index íŠ¹ë³„ ì²˜ë¦¬
                if is_hang_seng:
                    sync_logger.info(f"ğŸ‡­ğŸ‡° [Hang Seng Index] íŠ¹ë³„ ì²˜ë¦¬ ì‹œì‘: í‹°ì»¤ ì •ê·œí™” ì¤‘...")
                    
                    try:
                        resolved_tickers, unresolved_companies = _parse_hang_seng_index(html_content, sync_logger)
                        
                        # í•´ê²°ëœ í‹°ì»¤ ì¶”ê°€
                        for ticker in resolved_tickers:
                            all_tickers_map[ticker] = country
                        
                        tickers_from_this_source = len(resolved_tickers)
                        
                        if tickers_from_this_source > 0:
                            success_count += 1
                            error_details.append(
                                f"âœ… {url}: {tickers_from_this_source}ê°œ í‹°ì»¤ í•´ê²°"
                                + (f", {len(unresolved_companies)}ê°œ ë¯¸í•´ê²°" if unresolved_companies else "")
                            )
                        else:
                            fail_count += 1
                            error_details.append(
                                f"âš ï¸  {url}: í‹°ì»¤ í•´ê²° ì‹¤íŒ¨"
                                + (f" ({len(unresolved_companies)}ê°œ ì¢…ëª©)" if unresolved_companies else "")
                            )
                    except Exception as e:
                        fail_count += 1
                        import traceback
                        error_msg = f"âŒ {url}: Hang Seng Index íŒŒì‹± ì˜¤ë¥˜ - {type(e).__name__}: {str(e)[:200]}"
                        error_details.append(error_msg)
                        sync_logger.error(f"   {error_msg}")
                    
                    continue  # Hang Seng Index ì²˜ë¦¬ ì™„ë£Œ, ë‹¤ìŒ URLë¡œ
                
                # CSI 300 Index íŠ¹ë³„ ì²˜ë¦¬
                if is_csi_300:
                    sync_logger.info(f"ğŸ‡¨ğŸ‡³ [CSI 300 Index] íŠ¹ë³„ ì²˜ë¦¬ ì‹œì‘: í‹°ì»¤ ì •ê·œí™” ì¤‘...")
                    
                    try:
                        resolved_tickers, unresolved_companies = _parse_csi_300_index(html_content, sync_logger)
                        
                        # í•´ê²°ëœ í‹°ì»¤ ì¶”ê°€
                        for ticker in resolved_tickers:
                            all_tickers_map[ticker] = country
                        
                        tickers_from_this_source = len(resolved_tickers)
                        
                        if tickers_from_this_source > 0:
                            success_count += 1
                            error_details.append(
                                f"âœ… {url}: {tickers_from_this_source}ê°œ í‹°ì»¤ í•´ê²°"
                                + (f", {len(unresolved_companies)}ê°œ ë¯¸í•´ê²°" if unresolved_companies else "")
                            )
                        else:
                            fail_count += 1
                            error_details.append(
                                f"âš ï¸  {url}: í‹°ì»¤ í•´ê²° ì‹¤íŒ¨"
                                + (f" ({len(unresolved_companies)}ê°œ ì¢…ëª©)" if unresolved_companies else "")
                            )
                    except Exception as e:
                        fail_count += 1
                        import traceback
                        error_msg = f"âŒ {url}: CSI 300 Index íŒŒì‹± ì˜¤ë¥˜ - {type(e).__name__}: {str(e)[:200]}"
                        error_details.append(error_msg)
                        sync_logger.error(f"   {error_msg}")
                    
                    continue  # CSI 300 Index ì²˜ë¦¬ ì™„ë£Œ, ë‹¤ìŒ URLë¡œ
                
                # DAX íŠ¹ë³„ ì²˜ë¦¬
                if is_dax:
                    sync_logger.info(f"ğŸ‡©ğŸ‡ª [DAX] íŠ¹ë³„ ì²˜ë¦¬ ì‹œì‘: .DE íŒ¨í„´ ë§¤ì¹­ ì¤‘...")
                    
                    try:
                        resolved_tickers, unresolved_companies = _parse_dax(html_content, sync_logger)
                        
                        # í•´ê²°ëœ í‹°ì»¤ ì¶”ê°€
                        for ticker in resolved_tickers:
                            all_tickers_map[ticker] = country
                        
                        tickers_from_this_source = len(resolved_tickers)
                        
                        if tickers_from_this_source > 0:
                            success_count += 1
                            error_details.append(
                                f"âœ… {url}: {tickers_from_this_source}ê°œ í‹°ì»¤ í•´ê²°"
                                + (f", {len(unresolved_companies)}ê°œ ë¯¸í•´ê²°" if unresolved_companies else "")
                            )
                        else:
                            fail_count += 1
                            error_details.append(
                                f"âš ï¸  {url}: í‹°ì»¤ í•´ê²° ì‹¤íŒ¨"
                                + (f" ({len(unresolved_companies)}ê°œ ì¢…ëª©)" if unresolved_companies else "")
                            )
                    except Exception as e:
                        fail_count += 1
                        import traceback
                        error_msg = f"âŒ {url}: DAX íŒŒì‹± ì˜¤ë¥˜ - {type(e).__name__}: {str(e)[:200]}"
                        error_details.append(error_msg)
                        sync_logger.error(f"   {error_msg}")
                    
                    continue  # DAX ì²˜ë¦¬ ì™„ë£Œ, ë‹¤ìŒ URLë¡œ
                
                # ì¼ë°˜ ì§€ìˆ˜ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§)
                # pandas.read_htmlë¡œ HTML íŒŒì‹±
                # ì—¬ëŸ¬ íŒŒì„œ ì‹œë„ (lxml â†’ html5lib â†’ ê¸°ë³¸)
                tables = None
                parser_used = None
                last_error = None
                
                for parser_name in ['lxml', 'html5lib', None]:
                    try:
                        # StringIOë¡œ ê°ì‹¸ì„œ FutureWarning í•´ê²°
                        html_io = StringIO(html_content)
                        if parser_name:
                            tables = pd.read_html(html_io, flavor=parser_name)
                            parser_used = parser_name
                            break
                        else:
                            tables = pd.read_html(html_io)
                            parser_used = 'default'
                            break
                    except Exception as e:
                        last_error = str(e)
                        continue
                
                if tables is None:
                    raise Exception(f"ëª¨ë“  íŒŒì„œ ì‹¤íŒ¨. ë§ˆì§€ë§‰ ì˜¤ë¥˜: {last_error}")
                
                tickers_from_this_source = 0
                
                # ê° í…Œì´ë¸”ì—ì„œ í‹°ì»¤ ì»¬ëŸ¼ ì°¾ê¸°
                for table_idx, table in enumerate(tables):
                    # í‹°ì»¤ ì»¬ëŸ¼ ì´ë¦„ í›„ë³´ë“¤ (ë” ë§ì€ ë³€í˜• í¬í•¨)
                    ticker_columns = [
                        "Symbol", "Ticker", "Ticker symbol", "Symbols", "Code",
                        "Ticker symbol", "Ticker Symbol", "SYMBOL", "TICKER",
                        # S&P 500ì˜ ê²½ìš° "Symbol"ì´ ì²« ë²ˆì§¸ ì»¬ëŸ¼ì¼ ìˆ˜ ìˆìŒ
                    ]
                    
                    # MultiIndex ì»¬ëŸ¼ ì²˜ë¦¬ (íŠœí”Œë¡œ ëœ ì»¬ëŸ¼ëª…)
                    if isinstance(table.columns, pd.MultiIndex):
                        # ì²« ë²ˆì§¸ ë ˆë²¨ë§Œ ì‚¬ìš©
                        table.columns = table.columns.get_level_values(0)

                    # êµ­ê°€ ì •ë³´ë¥¼ ë‹´ê³  ìˆì„ ìˆ˜ ìˆëŠ” ì»¬ëŸ¼ í›„ë³´
                    country_columns = [
                        col for col in table.columns
                        if any(key in str(col).lower() for key in ["country", "headquarters", "headquarter", "location"])
                    ]
                    
                    def _process_row(row, ticker_value, row_country_override=None):
                        nonlocal tickers_from_this_source
                        if pd.isna(ticker_value):
                            return
                        ticker_str = str(ticker_value).strip().upper()
                        ticker_str = ticker_str.split()[0] if ticker_str else ""
                        ticker_str = ticker_str.replace("(", "").replace(")", "")
                        if not ticker_str:
                            return

                        # í–‰ ë‹¨ìœ„ êµ­ê°€ ì •ë³´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ country
                        row_country = row_country_override or country
                        if country_columns:
                            for ccol in country_columns:
                                try:
                                    cval = row.get(ccol)
                                except Exception:
                                    cval = None
                                if cval is not None and not pd.isna(cval):
                                    cval_str = str(cval).strip()
                                    if cval_str:
                                        row_country = cval_str
                                        break

                        normalized_ticker = _apply_country_suffix(ticker_str, row_country)
                        if normalized_ticker:
                            all_tickers_map[normalized_ticker] = row_country or country
                            tickers_from_this_source += 1
                    
                    found_column = None
                    for col_name in ticker_columns:
                        # ì •í™•í•œ ë§¤ì¹­ê³¼ ë¶€ë¶„ ë§¤ì¹­ ëª¨ë‘ ì‹œë„
                        matching_cols = [col for col in table.columns if str(col).upper() == col_name.upper() or col_name.upper() in str(col).upper()]
                        if matching_cols:
                            found_column = matching_cols[0]
                            # í‹°ì»¤ ì¶”ì¶œ ë° ì •ì œ
                            for _, row in table.iterrows():
                                try:
                                    _process_row(row, row[found_column])
                                except Exception:
                                    continue
                            break  # í‹°ì»¤ ì»¬ëŸ¼ì„ ì°¾ìœ¼ë©´ ë‹¤ìŒ í…Œì´ë¸”ë¡œ
                    
                    # í‹°ì»¤ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°, ì¸ë±ìŠ¤ 4ë²ˆì§¸ ì»¬ëŸ¼ í™•ì¸ (DAX ë“±)
                    if found_column is None and len(table.columns) > 4:
                        try:
                            # ì¸ë±ìŠ¤ 4 (0-ê¸°ë°˜)ì— Ticker ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
                            col_at_index_4 = table.columns[4]
                            if 'ticker' in str(col_at_index_4).lower() or 'symbol' in str(col_at_index_4).lower():
                                found_column = col_at_index_4
                                for _, row in table.iterrows():
                                    try:
                                        _process_row(row, row[found_column])
                                    except Exception:
                                        continue
                        except (IndexError, KeyError):
                            pass
                    
                    # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ì§§ì€ ë¬¸ìì—´ì´ê³  ìˆ«ìê°€ ì•„ë‹Œ ê²½ìš° í‹°ì»¤ì¼ ìˆ˜ ìˆìŒ (S&P 500 ë“±)
                    if found_column is None and len(table.columns) > 0:
                        first_col = table.columns[0]
                        # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì˜ ìƒ˜í”Œ ê°’ í™•ì¸
                        sample_values = table[first_col].dropna().head(5).astype(str)
                        # ëŒ€ë¶€ë¶„ì´ 1-5ì ê¸¸ì´ì˜ ì˜ë¬¸ ëŒ€ë¬¸ìì¸ ê²½ìš° í‹°ì»¤ì¼ ê°€ëŠ¥ì„±
                        if len(sample_values) > 0:
                            ticker_like = sum(1 for v in sample_values if 1 <= len(v.strip()) <= 5 and v.strip().isalpha())
                            if ticker_like >= len(sample_values) * 0.8:  # 80% ì´ìƒì´ í‹°ì»¤ì²˜ëŸ¼ ë³´ì´ë©´
                                found_column = first_col
                                for _, row in table.iterrows():
                                    try:
                                        _process_row(row, row[first_col])
                                    except Exception:
                                        continue
                
                if tickers_from_this_source > 0:
                    success_count += 1
                    error_details.append(f"âœ… {url}: {len(tables)}ê°œ í…Œì´ë¸” ({parser_used}), {tickers_from_this_source}ê°œ í‹°ì»¤ ì¶”ì¶œ")
                else:
                    fail_count += 1
                    # í…Œì´ë¸” ì»¬ëŸ¼ ì •ë³´ë„ ì¶œë ¥
                    table_info = []
                    for idx, table in enumerate(tables[:3]):  # ìµœëŒ€ 3ê°œ í…Œì´ë¸”ë§Œ
                        table_info.append(f"í…Œì´ë¸”{idx+1}: {list(table.columns)[:5]}")
                    error_details.append(f"âš ï¸  {url}: {len(tables)}ê°œ í…Œì´ë¸” ë°œê²¬í–ˆìœ¼ë‚˜ í‹°ì»¤ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í•¨ ({', '.join(table_info)})")
                
            except Exception as e:
                fail_count += 1
                import traceback
                error_msg = f"âŒ {url}: {type(e).__name__}: {str(e)[:200]}"
                error_details.append(error_msg)
                # ëª¨ë“  ì˜¤ë¥˜ì˜ ìƒì„¸ ì •ë³´ í¬í•¨ (ìµœëŒ€ 300ì)
                tb_str = traceback.format_exc()
                if len(tb_str) > 300:
                    tb_str = tb_str[:300] + "..."
                error_details.append(f"   ìƒì„¸: {tb_str}")
                continue
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        unique_tickers_map = dict(sorted(all_tickers_map.items()))
        
        return unique_tickers_map, {
            "success_count": success_count,
            "fail_count": fail_count,
            "total_tickers": len(unique_tickers_map),
            "error_details": error_details
        }

    result_map, details = await asyncio.to_thread(_sync_job)
    
    # ê²°ê³¼ ë¡œê¹…
    logger.info(f"   ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: ì„±ê³µ {details['success_count']}ê°œ, ì‹¤íŒ¨ {details['fail_count']}ê°œ")
    for detail in details['error_details']:
        logger.info(f"   {detail}")
    
    if len(result_map) > 0:
        logger.info(f"âœ… [Step 1] í‹°ì»¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(result_map)}ê°œ í‹°ì»¤ ìˆ˜ì§‘ë¨")
        # ìƒ˜í”Œ í‹°ì»¤ ë¡œê¹… ë³€ê²½
        sample_tickers = [f"{t} ({c})" for t, c in list(result_map.items())[:10]]
        logger.info(f"   ìƒ˜í”Œ í‹°ì»¤ (ìµœëŒ€ 10ê°œ): {', '.join(sample_tickers)}")
    else:
        logger.error(f"âŒ [Step 1] í‹°ì»¤ ìˆ˜ì§‘ ì‹¤íŒ¨: 0ê°œ í‹°ì»¤ ìˆ˜ì§‘ë¨")
        logger.error("   ê°€ëŠ¥í•œ ì›ì¸:")
        logger.error("     1. ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œ")
        logger.error("     2. ìœ„í‚¤í”¼ë””ì•„ í˜ì´ì§€ êµ¬ì¡° ë³€ê²½")
        logger.error("     3. pandas.read_html ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¬¸ì œ")
        logger.error("     4. ë°©í™”ë²½/í”„ë¡ì‹œ ì„¤ì • ë¬¸ì œ")
    
    return result_map


async def _fetch_single_ticker_yf(ticker: str) -> Optional[Dict[str, Any]]:
    """yfinanceë¡œ ë‹¨ì¼ í‹°ì»¤ì˜ ì‹œê°€ì´ì•¡ / ê°€ê²© / íšŒì‚¬ ì •ë³´ ì¡°íšŒ. (êµ­ê°€ ì •ë³´ ìˆ˜ì§‘ ë¡œì§ ì œê±°)"""

    def _sync_job() -> Optional[Dict[str, Any]]:
        import logging
        import random
        import time

        logger = logging.getLogger(__name__)
        max_attempts = 3
        last_error: Optional[Exception] = None

        for attempt in range(1, max_attempts + 1):
            info: Dict[str, Any] = {}
            try:
                t = yf.Ticker(ticker)
                info = t.info or {}
            except Exception as e:
                last_error = e
                logger.warning(
                    f"[yf] {ticker} info fetch failed (try {attempt}/{max_attempts}): "
                    f"{type(e).__name__}: {str(e)[:150]}"
                )

            market_cap = info.get("marketCap")
            price = info.get("currentPrice") or info.get("regularMarketPrice")

            if market_cap is not None or price is not None:
                data: Dict[str, Any] = {
                    "ticker": ticker,
                    "name": info.get("longName") or info.get("shortName") or ticker,
                    "sector": info.get("sector"),
                    "industry": info.get("industry"),
                    "currency": info.get("currency"),
                    # í˜„ì§€í†µí™” ê¸°ì¤€ ì‹œê°€ì´ì•¡ (ì›ë³¸)
                    "market_cap_local": float(market_cap) if market_cap is not None else None,
                    "price": float(price) if price is not None else None,
                    "volume": info.get("volume"),
                }
                # market_capëŠ” í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
                data["market_cap"] = data["market_cap_local"]
                return data

            if attempt < max_attempts:
                delay = random.uniform(1, 2)
                logger.warning(
                    f"[yf] {ticker} missing marketCap/price (try {attempt}/{max_attempts}); "
                    f"retrying in {delay:.1f}s"
                )
                time.sleep(delay)

        if last_error:
            logger.warning(
                f"[yf] {ticker} failed after {max_attempts} tries: "
                f"{type(last_error).__name__}: {str(last_error)[:150]}"
            )
        else:
            logger.warning(f"[yf] {ticker} missing marketCap/price after {max_attempts} tries")
        return None

    data = await asyncio.to_thread(_sync_job)
    if data is None:
        return None

    market_cap_local = data.get("market_cap_local")
    currency = data.get("currency") or "USD"

    if market_cap_local is not None:
        rate = await get_usd_exchange_rate(currency)
        data["market_cap_usd"] = market_cap_local * rate
    else:
        data["market_cap_usd"] = None

    return data


async def _fetch_company_logo_fmp(ticker: str, company_name: Optional[str] = None) -> Optional[str]:
    """
    ê¸°ì—… ë¡œê³  URLì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    ì „ëµ:
    - FMP Image API URLì„ ì§ì ‘ ì¡°ë¦½í•´ HEADë¡œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    - í•„ìš” ì‹œ GET(stream=True)ë¡œ Content-Typeì„ ì¬í™•ì¸
    - 200ì´ë©° Content-Typeì´ image/*ì´ë©´ ì„±ê³µ, ê·¸ ì™¸(404 í¬í•¨)ëŠ” ì‹¤íŒ¨ ì²˜ë¦¬
    """
    import logging
    logger = logging.getLogger(__name__)
    
    if not settings.fmp_api_key:
        logger.debug("FMP API Key ì—†ìŒ: ë¡œê³  ìˆ˜ì§‘ ê±´ë„ˆëœ€")
        return None
    
    def _try_fmp_image_api() -> Optional[str]:
        session = _get_fmp_session()
        normalized_ticker = ticker.upper()
        image_url = f"https://financialmodelingprep.com/image-stock/{normalized_ticker}.png?apikey={settings.fmp_api_key}"

        # 1ì°¨: HEADë¡œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (redirect í—ˆìš©)
        try:
            head_resp = session.head(image_url, allow_redirects=True, timeout=5)
            status = head_resp.status_code
            content_type = head_resp.headers.get("Content-Type", "").lower()
            head_resp.close()

            if status == 200 and content_type.startswith("image/"):
                logger.debug(f"FMP Direct URL Strategy: {normalized_ticker} HEAD í™•ì¸ ì„±ê³µ (ct={content_type})")
                return image_url
        except Exception as e:
            logger.debug(f"FMP Direct URL Strategy HEAD ì˜¤ë¥˜ ({ticker}): {type(e).__name__}: {str(e)[:100]}")

        # 2ì°¨: GET(stream=True)ë¡œ ì‹¤ì œ ì½˜í…ì¸  í™•ì¸
        try:
            with session.get(image_url, stream=True, timeout=5) as resp:
                status = resp.status_code
                content_type = resp.headers.get("Content-Type", "").lower()
                is_image = content_type.startswith("image/")

                if status == 200 and is_image:
                    logger.debug(f"FMP Direct URL Strategy: {normalized_ticker} ë¡œê³  ìˆ˜ì§‘ ì„±ê³µ (ct={content_type})")
                    return image_url

                logger.debug(f"FMP Direct URL Strategy: {normalized_ticker} ì‹¤íŒ¨ (status={status}, ct={content_type})")
                return None
        except Exception as e:
            logger.debug(f"FMP Direct URL Strategy ì˜¤ë¥˜ ({ticker}): {type(e).__name__}: {str(e)[:100]}")
            return None
    
    return await asyncio.to_thread(_try_fmp_image_api)


async def fetch_top_100_data(tickers_map: Dict[str, str]) -> List[Dict[str, Any]]:
    """
    ìˆ˜ì§‘ëœ í‹°ì»¤ ëª©ë¡ì— ëŒ€í•´ yfinanceë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°€ì´ì•¡ / ê°€ê²©ì„ ì¡°íšŒí•˜ê³ ,
    ì‹œê°€ì´ì•¡(USD ê¸°ì¤€) ìƒìœ„ 100ê°œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    FMP APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œê³  URLë„ í•¨ê»˜ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    import logging
    logger = logging.getLogger(__name__)
    
    tickers = list(tickers_map.keys())
    total_tickers = len(tickers)
    
    logger.info(f"ğŸ“Š [Step 2] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {total_tickers}ê°œ í‹°ì»¤ ìŠ¤ìº”")
    logger.info(f"   ì „ëµ: fast_info ìš°ì„  ìŠ¤ìº” (ë°°ì¹˜ ì²˜ë¦¬) â†’ Top 100 ìƒì„¸ ì¡°íšŒ")

    async def _fetch_fast_marketcap(ticker: str) -> Optional[Dict[str, Any]]:
        def _sync_fast() -> Optional[Dict[str, Any]]:
            try:
                t = yf.Ticker(ticker)
                fi = getattr(t, "fast_info", None)
                if not fi:
                    return None
                mc = getattr(fi, "market_cap", None)
                if mc is None:
                    return None
                return {
                    "ticker": ticker,
                    "market_cap_local": float(mc),
                    "price": getattr(fi, "last_price", None),
                    "currency": getattr(fi, "currency", None),
                    "volume": getattr(fi, "last_volume", None),
                    "name": ticker,
                    "sector": None,
                    "industry": None,
                }
            except Exception as e:
                logger.warning(f"âš ï¸ {ticker} fast_info ì‹¤íŒ¨: {e}")
                raise

        # fast_infoë„ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ì´ë¯€ë¡œ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
        fast = await asyncio.to_thread(_sync_fast)
        if not fast:
            return None

        currency = fast.get("currency") or "USD"
        rate = await get_usd_exchange_rate(currency)
        fast["market_cap_usd"] = fast["market_cap_local"] * rate
        fast["market_cap"] = fast["market_cap_local"]
        return fast

    # ---------------------------------------------------------
    # [ìˆ˜ì •ë¨] 1ì°¨: fast_info ìŠ¤ìº”ì—ë„ ë°°ì¹˜ ì²˜ë¦¬ ì ìš© (ê³¼ë¶€í•˜ ë°©ì§€)
    # ---------------------------------------------------------
    fast_valid: List[Dict[str, Any]] = []
    fast_map: Dict[str, Dict[str, Any]] = {}
    failed_fast_scan_tickers: List[str] = []
    
    # í•œ ë²ˆì— 50ê°œì”© ìŠ¤ìº”
    SCAN_BATCH_SIZE = 50
    
    for i in range(0, total_tickers, SCAN_BATCH_SIZE):
        batch_tickers = tickers[i : i + SCAN_BATCH_SIZE]
        logger.info(f"   ğŸ” 1ì°¨ ìŠ¤ìº” ë°°ì¹˜: {i+1}~{min(i+SCAN_BATCH_SIZE, total_tickers)}/{total_tickers}")
        
        tasks = [asyncio.create_task(_fetch_fast_marketcap(t)) for t in batch_tickers]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for idx, res in enumerate(results):
            if isinstance(res, Exception):
                failed_fast_scan_tickers.append(batch_tickers[idx])
                continue
            if res and res.get("market_cap_usd") is not None:
                fast_valid.append(res)
                fast_map[res["ticker"]] = res
        
        # ë°°ì¹˜ ê°„ ì§§ì€ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ì™„í™”)
        if i + SCAN_BATCH_SIZE < total_tickers:
            await asyncio.sleep(1.0)

    if not fast_valid:
        logger.error("âŒ fast_info ê¸°ë°˜ ìŠ¤ìº” ì‹¤íŒ¨: ìœ íš¨í•œ ì‹œê°€ì´ì•¡ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return []

    # ì‹œê°€ì´ì•¡ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    fast_valid.sort(key=lambda x: x["market_cap_usd"] or 0.0, reverse=True)
    top_fast = fast_valid[:100]
    top_fast_tickers = [item["ticker"] for item in top_fast]

    logger.info(f"âœ… 1ì°¨ ìŠ¤ìº” ì™„ë£Œ: ìœ íš¨ {len(fast_valid)}ê°œ ì¤‘ ìƒìœ„ 100ê°œ ì„ ì •")

    # ---------------------------------------------------------
    # 2ì°¨: Top 100ë§Œ ìƒì„¸ ì¡°íšŒ (í•„ìš” ì‹œ .info Fallback)
    # ---------------------------------------------------------
    DETAIL_BATCH_SIZE = 10  # ìƒì„¸ ì¡°íšŒëŠ” ë” ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ (10ê°œì”©)
    detailed_results: List[Optional[Dict[str, Any]]] = []
    
    detail_candidates = top_fast_tickers + [
        t for t in failed_fast_scan_tickers if t not in top_fast_tickers
    ]

    for batch_start in range(0, len(detail_candidates), DETAIL_BATCH_SIZE):
        batch_end = min(batch_start + DETAIL_BATCH_SIZE, len(detail_candidates))
        batch_tickers = detail_candidates[batch_start:batch_end]

        logger.info(f"   ğŸ“ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘: {batch_start + 1}~{batch_end}/{len(detail_candidates)}")

        async def _fetch_with_fallback(ticker: str):
            try:
                # ìƒì„¸ ì •ë³´(.info) ì‹œë„ -> ì‹¤íŒ¨ ì‹œ fast_info ë°ì´í„° ì‚¬ìš© (ìˆœìœ„ ìœ ì§€ ëª©ì )
                detailed = await _fetch_single_ticker_yf(ticker)
                return detailed or fast_map.get(ticker)
            except Exception as e:
                return fast_map.get(ticker)

        batch_tasks = [asyncio.create_task(_fetch_with_fallback(t)) for t in batch_tickers]
        batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

        for idx, res in enumerate(batch_results):
            if isinstance(res, Exception):
                res = fast_map.get(batch_tickers[idx])
            detailed_results.append(res)

        # ìƒì„¸ ì¡°íšŒ ë°°ì¹˜ ê°„ ëŒ€ê¸° (Rate Limit ë°©ì§€)
        if batch_end < len(top_fast_tickers):
            await asyncio.sleep(2.0)

    # ìœ íš¨ ë°ì´í„° í•„í„°ë§
    valid_items: List[Dict[str, Any]] = [
        r for r in detailed_results
        if r is not None and r.get("market_cap_usd") is not None
    ]

    valid_items.sort(key=lambda x: x["market_cap_usd"] or 0.0, reverse=True)
    top_100 = valid_items[:100]

    # êµ­ê°€ ì •ë³´ ë§¤í•‘
    country_count = 0
    for item in top_100:
        ticker = item["ticker"]
        country = tickers_map.get(ticker)
        if country:
            item["country"] = country
            country_count += 1
    
    # ---------------------------------------------------------
    # 3ì°¨: ë¡œê³  ìˆ˜ì§‘ (FMP)
    # ---------------------------------------------------------
    if top_100:
        logger.info(f"ğŸ–¼ï¸  [Step 3] ë¡œê³  ìˆ˜ì§‘ ì‹œì‘ ({len(top_100)}ê°œ)")
        
        LOGO_BATCH_SIZE = 10
        logo_results = []
        
        for batch_start in range(0, len(top_100), LOGO_BATCH_SIZE):
            batch_end = min(batch_start + LOGO_BATCH_SIZE, len(top_100))
            batch_items = top_100[batch_start:batch_end]
            
            # ë³‘ë ¬ ì²˜ë¦¬
            tasks = [
                asyncio.create_task(_fetch_company_logo_fmp(item["ticker"], item.get("name")))
                for item in batch_items
            ]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for res in results:
                if isinstance(res, Exception):
                    logo_results.append(None)
                else:
                    logo_results.append(res)
            
            if batch_end < len(top_100):
                await asyncio.sleep(0.2)
        
        # ê²°ê³¼ ë§¤í•‘
        for item, logo_url in zip(top_100, logo_results):
            item["logo_url"] = logo_url

    logger.info(f"âœ… [Step 2] ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: ìµœì¢… {len(top_100)}ê°œ")
    return top_100


async def update_rankings_db(
    top_100_list: List[Dict[str, Any]],
    db: AsyncSession,
) -> None:
    """
    ìƒìœ„ 100ê°œ ê¸°ì—… ì •ë³´ë¥¼ DBì— ì €ì¥/ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

    - companies: ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„± (insert), ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë‘ê±°ë‚˜ ì¼ë¶€ í•„ë“œ ì—…ë°ì´íŠ¸
    - rankings: year=í˜„ì¬ë…„ë„ ê¸°ì¤€ìœ¼ë¡œ ìˆœìœ„ ì •ë³´ upsert (ê°„ë‹¨íˆ ê¸°ì¡´ ì—°ë„ ë°ì´í„° ì‚­ì œ í›„ ì¬ì‚½ì…)
    - prices: ë‹¹ì¼ ê¸°ì¤€ ê°€ê²©/ì‹œê°€ì´ì•¡ ì €ì¥ (í‹°ì»¤+ë‚ ì§œ unique ê¸°ì¤€ upsert)
    """

    if not top_100_list:
        return

    current_year = datetime.now(timezone.utc).year
    today = datetime.now(timezone.utc).date()
    price_date = datetime(today.year, today.month, today.day, tzinfo=timezone.utc)

    # 1) Company upsert (ë°°ì¹˜ë¡œ ê¸°ì¡´ í‹°ì»¤ ì¡°íšŒ í›„, ì—†ëŠ” ê²ƒë§Œ insert)
    tickers = [item["ticker"] for item in top_100_list]

    stmt = select(models.Company).where(models.Company.ticker.in_(tickers))
    result = await db.execute(stmt)
    existing_companies = {c.ticker: c for c in result.scalars().all()}

    for item in top_100_list:
        ticker = item["ticker"]
        if ticker in existing_companies:
            company = existing_companies[ticker]
            # ê¸°ë³¸ ì •ë³´ëŠ” ìµœì‹  ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
            company.name = item.get("name") or company.name
            company.sector = item.get("sector") or company.sector
            company.industry = item.get("industry") or company.industry
            # country ì—…ë°ì´íŠ¸: itemì— countryê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸ (Noneì´ì–´ë„ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •)
            if "country" in item:
                company.country = item.get("country")
            company.currency = item.get("currency") or company.currency
            # ë¡œê³  URL ì—…ë°ì´íŠ¸: itemì— logo_urlì´ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸ (Noneì´ì–´ë„ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •)
            if "logo_url" in item:
                company.logo_url = item.get("logo_url")
        else:
            company = models.Company(
                ticker=ticker,
                name=item.get("name") or ticker,
                sector=item.get("sector"),
                industry=item.get("industry"),
                country=item.get("country"),
                currency=item.get("currency"),
                logo_url=item.get("logo_url"),
            )
            db.add(company)

    # 2) Rankings: í•´ë‹¹ ì—°ë„ ë°ì´í„° ì „ì²´ ì‚­ì œ í›„, 1~100ìœ„ ì¬ì‚½ì…
    await db.execute(
        delete(models.Ranking).where(models.Ranking.year == current_year)
    )

    for rank, item in enumerate(top_100_list, start=1):
        ranking = models.Ranking(
            year=current_year,
            rank=rank,
            ticker=item["ticker"],
            market_cap=item.get("market_cap_usd"),
            company_name=item.get("name") or item["ticker"],
        )
        db.add(ranking)

    # 3) Prices: (ticker, date) ê¸°ì¤€ upsert
    for item in top_100_list:
        ticker = item["ticker"]

        stmt = select(models.Price).where(
            models.Price.ticker == ticker,
            models.Price.date == price_date,
        )
        result = await db.execute(stmt)
        existing_price = result.scalar_one_or_none()

        if existing_price:
            existing_price.close = item.get("price")
            existing_price.market_cap = item.get("market_cap_usd")
            existing_price.volume = item.get("volume")
        else:
            price = models.Price(
                ticker=ticker,
                date=price_date,
                close=item.get("price"),
                market_cap=item.get("market_cap_usd"),
                volume=item.get("volume"),
            )
            db.add(price)

    await db.commit()


async def _calculate_ranking_changes(
    db: AsyncSession,
    current_top_100: List[Dict[str, Any]],
    ranking_date: date,
) -> Dict[str, Any]:
    """
    ì´ë²ˆ ë­í‚¹ê³¼ ê°€ì¥ ìµœê·¼ì˜ ê³¼ê±° ë­í‚¹ì„ ë¹„êµí•´ ë³€ë™ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    current_tickers: Set[str] = {item["ticker"] for item in current_top_100}

    # ì„¹í„°ë³„ í†µê³„ ì§‘ê³„ (ê°’ì´ ì—†ìœ¼ë©´ Unknownìœ¼ë¡œ ë¶„ë¥˜)
    sector_stats: Dict[str, int] = {}
    for item in current_top_100:
        sector = item.get("sector") or "Unknown"
        sector_stats[sector] = sector_stats.get(sector, 0) + 1

    # 1) ranking_date ê¸°ì¤€ ì§ì „ ë°ì´í„° ë‚ ì§œ ì¡°íšŒ
    latest_past_date_stmt = (
        select(models.Ranking.ranking_date)
        .where(models.Ranking.ranking_date.is_not(None))
        .where(models.Ranking.ranking_date < ranking_date)
        .order_by(models.Ranking.ranking_date.desc())
        .limit(1)
    )
    result = await db.execute(latest_past_date_stmt)
    latest_past_date = result.scalar_one_or_none()

    previous_tickers: Set[str] = set()

    if latest_past_date:
        prev_stmt = select(models.Ranking.ticker).where(
            models.Ranking.ranking_date == latest_past_date
        )
        prev_result = await db.execute(prev_stmt)
        previous_tickers = set(prev_result.scalars().all())
    else:
        # ranking_dateê°€ ì—†ëŠ” ê¸°ì¡´ ì—°/ì›” ë°ì´í„° í˜¸í™˜ìš©: ì§ì „ ì—°ë„ì˜ ë­í‚¹ì„ ì‚¬ìš©
        fallback_year_stmt = (
            select(models.Ranking.year)
            .where(models.Ranking.year < ranking_date.year)
            .order_by(models.Ranking.year.desc())
            .limit(1)
        )
        fallback_year_result = await db.execute(fallback_year_stmt)
        fallback_year = fallback_year_result.scalar_one_or_none()

        if fallback_year is not None:
            prev_stmt = select(models.Ranking.ticker).where(
                models.Ranking.year == fallback_year
            )
            prev_result = await db.execute(prev_stmt)
            previous_tickers = set(prev_result.scalars().all())

    new_entries = sorted(current_tickers - previous_tickers)
    exited = sorted(previous_tickers - current_tickers)

    return {
        "previous_ranking_date": latest_past_date,
        "new_entries": new_entries,
        "exited": exited,
        "sector_stats": sector_stats,
    }


async def collect_and_update_global_top_100(db: AsyncSession, limit: int = None) -> Dict[str, Any]:
    """
    ê¸€ë¡œë²Œ ìƒìœ„ 100ê°œ ê¸°ì—… ìˆ˜ì§‘ ë° DB ì—…ë°ì´íŠ¸ (Company, Ranking, Price)
    """
    import logging
    logger = logging.getLogger(__name__)
    
    logger.info("="*70)
    logger.info("ğŸš€ [ê¸€ë¡œë²Œ ìƒìœ„ 100ê°œ ê¸°ì—… ì¬ì¡°ì‚¬] ì‹œì‘")
    
    # 1. í‹°ì»¤ ìˆ˜ì§‘
    tickers_map = await fetch_index_tickers()
    tickers = list(tickers_map.keys())
    
    if limit and limit > 0:
        tickers = tickers[:limit]
        tickers_map = {t: tickers_map[t] for t in tickers if t in tickers_map}
        logger.info(f"   âš ï¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {limit}ê°œ í‹°ì»¤ë§Œ ì²˜ë¦¬")

    if not tickers:
        logger.error("âŒ í‹°ì»¤ ìˆ˜ì§‘ ì‹¤íŒ¨")
        return {"top_100": [], "ranking_date": datetime.now(timezone.utc).date(), "changes": {}}

    # 2. ë°ì´í„° ìˆ˜ì§‘ (yfinance + FMP logo)
    top_100 = await fetch_top_100_data(tickers_map)
    ranking_date = datetime.now(timezone.utc).date()
    
    if not top_100:
        logger.error("âŒ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ (ê²°ê³¼ ì—†ìŒ)")
        return {"top_100": [], "ranking_date": ranking_date, "changes": {}}

    # 3. ë³€ë™ ë°ì´í„° ê³„ì‚°
    logger.info(f"ğŸ“Š [Step 4-1] ë­í‚¹ ë³€ë™ ê³„ì‚° ì¤‘...")
    changes = await _calculate_ranking_changes(db, top_100, ranking_date)

    # 4. DB ì €ì¥ ì‹œì‘
    logger.info(f"ğŸ’¾ [Step 4] DB ì €ì¥ íŠ¸ëœì­ì…˜ ì‹œì‘ (ê¸°ì—…: {len(top_100)}ê°œ)")
    
    try:
        current_year = ranking_date.year
        tickers_list = [item["ticker"] for item in top_100]

        # [4-2] Company Update
        stmt = select(models.Company).where(models.Company.ticker.in_(tickers_list))
        result = await db.execute(stmt)
        existing_companies = {c.ticker: c for c in result.scalars().all()}
        
        for item in top_100:
            ticker = item["ticker"]
            if ticker in existing_companies:
                c = existing_companies[ticker]
                c.name = item.get("name") or c.name
                c.sector = item.get("sector") or c.sector
                c.industry = item.get("industry") or c.industry
                c.currency = item.get("currency") or c.currency
                if item.get("country"): c.country = item.get("country")
                if item.get("logo_url"): c.logo_url = item.get("logo_url")
            else:
                new_c = models.Company(
                    ticker=ticker,
                    name=item.get("name") or ticker,
                    sector=item.get("sector"),
                    industry=item.get("industry"),
                    country=item.get("country"),
                    currency=item.get("currency"),
                    logo_url=item.get("logo_url"),
                )
                db.add(new_c)
        
        # [4-3] Rankings Update (Delete & Insert)
        logger.info(f"ğŸ’¾ [Step 4-3] Rankings ì—…ë°ì´íŠ¸ (Date: {ranking_date})")
        await db.execute(delete(models.Ranking).where(models.Ranking.ranking_date == ranking_date))
        
        for rank, item in enumerate(top_100, start=1):
            ranking = models.Ranking(
                year=current_year,
                ranking_date=ranking_date,
                rank=rank,
                ticker=item["ticker"],
                market_cap=item.get("market_cap_usd"),
                company_name=item.get("name") or item["ticker"],
            )
            db.add(ranking)

        # [4-4] Prices Update (Upsert)
        logger.info(f"ğŸ’¾ [Step 4-4] Prices ì—…ë°ì´íŠ¸ (Date: {ranking_date})")
        price_datetime = datetime(ranking_date.year, ranking_date.month, ranking_date.day, tzinfo=timezone.utc)
        
        # ì¼ê´„ ì²˜ë¦¬ë¥¼ ìœ„í•´ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ (ì„±ëŠ¥ ìµœì í™”)
        price_stmt = select(models.Price).where(
            models.Price.ticker.in_(tickers_list),
            models.Price.date == price_datetime
        )
        price_result = await db.execute(price_stmt)
        existing_prices = {p.ticker: p for p in price_result.scalars().all()}
        
        prices_added = 0
        prices_updated = 0
        
        for item in top_100:
            ticker = item["ticker"]
            if ticker in existing_prices:
                p = existing_prices[ticker]
                p.close = item.get("price")
                p.market_cap = item.get("market_cap_usd")
                p.volume = item.get("volume")
                prices_updated += 1
            else:
                new_p = models.Price(
                    ticker=ticker,
                    date=price_datetime,
                    close=item.get("price"),
                    market_cap=item.get("market_cap_usd"),
                    volume=item.get("volume")
                )
                db.add(new_p)
                prices_added += 1
        
        logger.info(f"   Prices ê²°ê³¼: ì‹ ê·œ {prices_added}ê°œ, ì—…ë°ì´íŠ¸ {prices_updated}ê°œ")

        # [4-5] íŠ¸ëœì­ì…˜ ì»¤ë°‹
        await db.commit()
        logger.info("âœ… [Step 4] DB ì €ì¥ ì™„ë£Œ!")

        # [4-6] AI íŠ¸ë Œë“œ ë¶„ì„ (ì €ì¥ í›„ ì‹¤í–‰)
        try:
            ai_trend_text = await ai_client.generate_sector_trend_analysis(changes)
            sector_trend = models.SectorTrend(
                date=ranking_date,
                dominant_sectors=changes.get("sector_stats"),
                new_entries={"new": changes.get("new_entries"), "exited": changes.get("exited")},
                ai_analysis_text=ai_trend_text,
            )
            db.add(sector_trend)
            await db.commit()
            logger.info("âœ… ì„¹í„° íŠ¸ë Œë“œ AI ë¶„ì„ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ ì„¹í„° íŠ¸ë Œë“œ ì €ì¥ ì‹¤íŒ¨: {e}")

    except Exception as e:
        logger.error(f"âŒ DB ì €ì¥ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        await db.rollback()
        import traceback
        logger.error(traceback.format_exc())
        return {"top_100": [], "ranking_date": ranking_date, "changes": {}}
    
    return {
        "top_100": top_100,
        "ranking_date": ranking_date,
        "changes": changes,
    }


async def _process_single_ticker_news(
    ticker: str,
) -> tuple[str, dict | None, bool]:
    """
    ë‹¨ì¼ í‹°ì»¤ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬ìš©)
    DB ì„¸ì…˜ì€ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë°ì´í„°ë§Œ ìˆ˜ì§‘í•˜ì—¬ ë°˜í™˜.
    
    Returns:
        (ticker, report_data, success)
    """
    try:
        # ë‰´ìŠ¤ì™€ ì¬ë¬´ ë°ì´í„°ë¥¼ ë³‘ë ¬ë¡œ ìˆ˜ì§‘
        news_task = news_service.fetch_company_news(ticker, limit=5)
        stock_task = stock_service.fetch_company_data(ticker)
        
        news_list, stock_data = await asyncio.gather(
            news_task,
            stock_task,
            return_exceptions=True
        )
        
        # ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ ì—¬ë¶€ í™•ì¸
        news_failed = isinstance(news_list, Exception)
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        if isinstance(news_list, Exception):
            news_list = []
        if isinstance(stock_data, Exception):
            stock_data = {"financials": []}
        
        # ë‰´ìŠ¤ê°€ ì—†ê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš° DB ì €ì¥ ê±´ë„ˆë›°ê¸°
        if news_failed or not news_list or len(news_list) == 0:
            return (ticker, None, False)
        
        # AI ë¶„ì„ (ë‰´ìŠ¤ê°€ ìˆì„ ë•Œë§Œ)
        ai_result = {
            "summary": "No recent news available",
            "sentiment_score": 0.0
        }
        
        if news_list and stock_data.get("financials"):
            try:
                financials_list = stock_data["financials"]
                latest_financial = financials_list[-1] if financials_list else {}
                
                ai_result = await ai_client.generate_market_summary(
                    ticker=ticker,
                    financials=latest_financial,
                    news_list=news_list,
                )
            except Exception:
                # AI ë¶„ì„ ì‹¤íŒ¨í•´ë„ ë‰´ìŠ¤ëŠ” ì €ì¥
                pass
        
        # raw_data êµ¬ì„± (ë‰´ìŠ¤ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
        raw_data_parts = []
        for news in news_list:
            title = news.get("title", "")
            url = news.get("url", "")
            source = news.get("source", "")
            news_date = news.get("date", "")
            raw_data_parts.append(f"Title: {title}\nSource: {source} ({news_date})\nLink: {url}")
        raw_data = "\n\n---\n\n".join(raw_data_parts)
        
        return (ticker, {
            "raw_data": raw_data,
            "summary_content": ai_result.get("summary"),
            "sentiment_score": ai_result.get("sentiment_score"),
        }, True)
        
    except Exception as e:
        import logging
        logger = logging.getLogger(__name__)
        logger.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ ({ticker}): {type(e).__name__}: {e}")
        return (ticker, None, False)


async def collect_news_for_top_100(db: AsyncSession) -> int:
    """
    ìƒìœ„ 100ê°œ ê¸°ì—…ì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ì—¬ MarketReportì— ì €ì¥í•©ë‹ˆë‹¤. (ì¼ë³„ ì‹¤í–‰)
    ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ìµœì í™”.
    
    Returns:
        ìˆ˜ì§‘í•œ ê¸°ì—… ìˆ˜
    """
    import logging
    logger = logging.getLogger(__name__)
    
    # í˜„ì¬ ìƒìœ„ 100ê°œ ê¸°ì—… ì¡°íšŒ
    current_year = datetime.now(timezone.utc).year
    stmt = select(models.Ranking).where(
        models.Ranking.year == current_year
    ).order_by(models.Ranking.rank).limit(100)
    result = await db.execute(stmt)
    rankings = result.scalars().all()
    
    if not rankings:
        return 0
    
    tickers = [r.ticker for r in rankings]
    today_utc = datetime.now(timezone.utc).date()
    today_start = datetime.combine(today_utc, datetime.min.time()).replace(tzinfo=timezone.utc)
    today_end = datetime.combine(today_utc, datetime.max.time()).replace(tzinfo=timezone.utc)
    
    logger.info(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {len(tickers)}ê°œ ê¸°ì—…")
    
    # ë°°ì¹˜ í¬ê¸° ì„¤ì • (ë™ì‹œì— ì²˜ë¦¬í•  ê¸°ì—… ìˆ˜) - TPM í•œë„ ê³ ë ¤í•˜ì—¬ 5ë¡œ ì„¤ì •
    BATCH_SIZE = 5
    collected_count = 0
    failed_count = 0
    
    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
    for batch_start in range(0, len(tickers), BATCH_SIZE):
        batch_end = min(batch_start + BATCH_SIZE, len(tickers))
        batch_tickers = tickers[batch_start:batch_end]
        
        logger.info(f"ë°°ì¹˜ ì²˜ë¦¬: {batch_start + 1}~{batch_end}/{len(tickers)}")
        
        # ë°°ì¹˜ ë‚´ì—ì„œ ë³‘ë ¬ ì²˜ë¦¬ (DB ì„¸ì…˜ ì—†ì´ ë°ì´í„°ë§Œ ìˆ˜ì§‘)
        tasks = [
            _process_single_ticker_news(ticker)
            for ticker in batch_tickers
        ]
        batch_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ë¥¼ DBì— ì €ì¥ (ìˆœì°¨ì ìœ¼ë¡œ)
        for result in batch_results:
            if isinstance(result, Exception):
                failed_count += 1
                continue
            
            ticker, report_data, success = result
            
            if not success or report_data is None:
                failed_count += 1
                continue
            
            try:
                # ê¸°ì¡´ ë¦¬í¬íŠ¸ í™•ì¸
                stmt = select(models.MarketReport).where(
                    models.MarketReport.ticker == ticker,
                    models.MarketReport.source_type == "daily_update",
                    models.MarketReport.collected_at >= today_start,
                    models.MarketReport.collected_at <= today_end,
                )
                result = await db.execute(stmt)
                existing_report = result.scalar_one_or_none()
                
                if existing_report:
                    # ì—…ë°ì´íŠ¸
                    existing_report.raw_data = report_data["raw_data"]
                    existing_report.summary_content = report_data["summary_content"]
                    existing_report.sentiment_score = report_data["sentiment_score"]
                    existing_report.content = "See raw_data or summary_content"
                else:
                    # ì‹ ê·œ ìƒì„±
                    report = models.MarketReport(
                        ticker=ticker,
                        source_type="daily_update",
                        raw_data=report_data["raw_data"],
                        summary_content=report_data["summary_content"],
                        sentiment_score=report_data["sentiment_score"],
                        content="See raw_data or summary_content"
                    )
                    db.add(report)
                
                collected_count += 1
                
            except Exception as e:
                logger.error(f"ë‰´ìŠ¤ ì €ì¥ ì‹¤íŒ¨ ({ticker}): {type(e).__name__}: {e}")
                failed_count += 1
                continue
        
        # ë°°ì¹˜ë§ˆë‹¤ ì»¤ë°‹
        try:
            await db.commit()
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì»¤ë°‹ ì‹¤íŒ¨: {type(e).__name__}: {e}")
            await db.rollback()
        
        # ë°°ì¹˜ ê°„ ëŒ€ê¸° (Rate Limit ë°©ì§€)
        if batch_end < len(tickers):
            await asyncio.sleep(2)
    
    logger.info(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {collected_count}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨")
    return collected_count


async def collect_daily_prices(db: AsyncSession) -> int:
    """
    ìƒìœ„ 100ê°œ ê¸°ì—…ì˜ ì¼ë³„ ì£¼ê°€Â·ì‹œê°€ì´ì•¡Â·ê±°ë˜ëŸ‰ì„ ìˆ˜ì§‘í•˜ì—¬ Price í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤. (ì¼ë³„ ì‹¤í–‰)
    
    Returns:
        ìˆ˜ì§‘í•œ ê¸°ì—… ìˆ˜
    """
    import logging
    logger = logging.getLogger(__name__)
    
    # í˜„ì¬ ìƒìœ„ 100ê°œ ê¸°ì—… ì¡°íšŒ
    current_year = datetime.now(timezone.utc).year
    stmt = select(models.Ranking).where(
        models.Ranking.year == current_year
    ).order_by(models.Ranking.rank).limit(100)
    result = await db.execute(stmt)
    rankings = result.scalars().all()
    
    if not rankings:
        return 0
    
    tickers = [r.ticker for r in rankings]
    today = datetime.now(timezone.utc).date()
    price_date = datetime(today.year, today.month, today.day, tzinfo=timezone.utc)
    
    logger.info(f"ì¼ë³„ ì£¼ê°€ ìˆ˜ì§‘ ì‹œì‘: {len(tickers)}ê°œ ê¸°ì—…")
    
    collected_count = 0
    
    # ë³‘ë ¬ë¡œ ë°ì´í„° ìˆ˜ì§‘
    tasks = [asyncio.create_task(_fetch_single_ticker_yf(t)) for t in tickers]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    for ticker, result in zip(tickers, results):
        if isinstance(result, Exception) or result is None:
            continue
        
        try:
            # ê¸°ì¡´ ê°€ê²© ë°ì´í„° í™•ì¸
            stmt = select(models.Price).where(
                models.Price.ticker == ticker,
                models.Price.date == price_date,
            )
            result_query = await db.execute(stmt)
            existing_price = result_query.scalar_one_or_none()
            
            if existing_price:
                # ì—…ë°ì´íŠ¸
                existing_price.close = result.get("price")
                existing_price.market_cap = result.get("market_cap_usd")
                existing_price.volume = result.get("volume")
            else:
                # ì‹ ê·œ ìƒì„±
                price = models.Price(
                    ticker=ticker,
                    date=price_date,
                    close=result.get("price"),
                    market_cap=result.get("market_cap_usd"),
                    volume=result.get("volume"),
                )
                db.add(price)
            
            collected_count += 1
            
        except Exception as e:
            logger.error(f"ì£¼ê°€ ì €ì¥ ì‹¤íŒ¨ ({ticker}): {type(e).__name__}: {e}")
            continue
    
    await db.commit()
    logger.info(f"ì¼ë³„ ì£¼ê°€ ìˆ˜ì§‘ ì™„ë£Œ: {collected_count}ê°œ")
    return collected_count


async def collect_quarterly_financials(db: AsyncSession) -> int:
    """
    ìƒìœ„ 100ê°œ ê¸°ì—…ì˜ ë¶„ê¸°ë³„ ì¬ë¬´ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ Financial í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤. (ë¶„ê¸°ë³„ ì‹¤í–‰)
    
    Returns:
        ìˆ˜ì§‘í•œ ê¸°ì—… ìˆ˜
    """
    # í˜„ì¬ ìƒìœ„ 100ê°œ ê¸°ì—… ì¡°íšŒ
    current_year = datetime.now(timezone.utc).year
    stmt = select(models.Ranking).where(
        models.Ranking.year == current_year
    ).order_by(models.Ranking.rank).limit(100)
    result = await db.execute(stmt)
    rankings = result.scalars().all()
    
    if not rankings:
        return 0
    
    tickers = [r.ticker for r in rankings]
    collected_count = 0
    
    # í˜„ì¬ ë¶„ê¸° ê³„ì‚° (1~4)
    now = datetime.now(timezone.utc)
    current_quarter = (now.month - 1) // 3 + 1
    
    for ticker in tickers:
        try:
            stock_data = await stock_service.fetch_company_data(ticker)
            
            if not stock_data.get("financials"):
                continue
            
            # ìµœì‹  ì¬ë¬´ ë°ì´í„°ë¥¼ í˜„ì¬ ë¶„ê¸°ë¡œ ì €ì¥
            latest_financial = stock_data["financials"][-1] if stock_data["financials"] else None
            
            if latest_financial:
                stmt = select(models.Financial).where(
                    models.Financial.ticker == ticker,
                    models.Financial.year == latest_financial["year"],
                    models.Financial.quarter == current_quarter
                )
                result = await db.execute(stmt)
                existing_fin = result.scalar_one_or_none()
                
                if existing_fin:
                    existing_fin.revenue = latest_financial.get("revenue")
                    existing_fin.net_income = latest_financial.get("net_income")
                    existing_fin.per = latest_financial.get("per")
                    existing_fin.market_cap = latest_financial.get("market_cap")
                else:
                    financial = models.Financial(
                        ticker=ticker,
                        year=latest_financial["year"],
                        quarter=current_quarter,
                        revenue=latest_financial.get("revenue"),
                        net_income=latest_financial.get("net_income"),
                        per=latest_financial.get("per"),
                        market_cap=latest_financial.get("market_cap"),
                    )
                    db.add(financial)
                
                collected_count += 1
                
        except Exception:
            continue
    
    await db.commit()
    return collected_count


async def collect_quarterly_reports(db: AsyncSession) -> int:
    """
    ìƒìœ„ 100ê°œ ê¸°ì—…ì˜ ë¶„ê¸°ë³„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ QuarterlyReport í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤. (ë¶„ê¸°ë³„ ì‹¤í–‰)
    
    Returns:
        ìƒì„±í•œ ë¦¬í¬íŠ¸ ìˆ˜
    """
    import logging
    logger = logging.getLogger(__name__)
    
    # í˜„ì¬ ìƒìœ„ 100ê°œ ê¸°ì—… ì¡°íšŒ
    current_year = datetime.now(timezone.utc).year
    stmt = select(models.Ranking).where(
        models.Ranking.year == current_year
    ).order_by(models.Ranking.rank).limit(100)
    result = await db.execute(stmt)
    rankings = result.scalars().all()
    
    if not rankings:
        return 0
    
    tickers = [r.ticker for r in rankings]
    
    # í˜„ì¬ ë¶„ê¸° ê³„ì‚° (1~4)
    now = datetime.now(timezone.utc)
    current_quarter = (now.month - 1) // 3 + 1
    
    logger.info(f"ë¶„ê¸°ë³„ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘: {len(tickers)}ê°œ ê¸°ì—…, {current_year}ë…„ {current_quarter}ë¶„ê¸°")
    
    generated_count = 0
    failed_count = 0
    
    for ticker in tickers:
        try:
            # ê¸°ì¡´ ë¦¬í¬íŠ¸ í™•ì¸
            stmt = select(models.QuarterlyReport).where(
                models.QuarterlyReport.ticker == ticker,
                models.QuarterlyReport.year == current_year,
                models.QuarterlyReport.quarter == current_quarter
            )
            result = await db.execute(stmt)
            existing_report = result.scalar_one_or_none()
            
            # ì´ë¯¸ ë¦¬í¬íŠ¸ê°€ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
            if existing_report:
                continue
            
            # ì¬ë¬´ ë°ì´í„° ì¡°íšŒ
            stmt = select(models.Financial).where(
                models.Financial.ticker == ticker,
                models.Financial.year == current_year,
                models.Financial.quarter == current_quarter
            )
            result = await db.execute(stmt)
            financial = result.scalar_one_or_none()
            
            if not financial:
                # ì¬ë¬´ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                continue
            
            # ì¬ë¬´ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
            financials_dict = {
                "year": financial.year,
                "revenue": financial.revenue,
                "net_income": financial.net_income,
                "per": financial.per,
                "market_cap": financial.market_cap,
            }
            
            # ìµœê·¼ ë‰´ìŠ¤ ì¡°íšŒ (ìµœê·¼ 3ê°œì›”)
            three_months_ago = datetime.now(timezone.utc) - timedelta(days=90)
            stmt = select(models.MarketReport).where(
                models.MarketReport.ticker == ticker,
                models.MarketReport.collected_at >= three_months_ago
            ).order_by(models.MarketReport.collected_at.desc()).limit(5)
            result = await db.execute(stmt)
            market_reports = result.scalars().all()
            
            # ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
            news_list = []
            for report in market_reports:
                if report.summary_content:
                    news_list.append({
                        "title": f"Market Report ({report.collected_at.date()})",
                        "body": report.summary_content,
                        "date": report.collected_at.isoformat(),
                    })
            
            # AIë¡œ ë¶„ê¸°ë³„ ë¦¬í¬íŠ¸ ìƒì„±
            try:
                report_content = await ai_client.generate_quarterly_report(
                    ticker=ticker,
                    year=current_year,
                    quarter=current_quarter,
                    financials=financials_dict,
                    news_list=news_list if news_list else None,
                )
                
                # QuarterlyReport ì €ì¥
                quarterly_report = models.QuarterlyReport(
                    ticker=ticker,
                    year=current_year,
                    quarter=current_quarter,
                    content=report_content,
                )
                db.add(quarterly_report)
                generated_count += 1
                
            except Exception as e:
                logger.error(f"ë¶„ê¸°ë³„ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨ ({ticker}): {type(e).__name__}: {e}")
                failed_count += 1
                continue
                
        except Exception as e:
            logger.error(f"ë¶„ê¸°ë³„ ë¦¬í¬íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨ ({ticker}): {type(e).__name__}: {e}")
            failed_count += 1
            continue
    
    await db.commit()
    logger.info(f"ë¶„ê¸°ë³„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {generated_count}ê°œ ìƒì„±, {failed_count}ê°œ ì‹¤íŒ¨")
    return generated_count
